Using 32
--------------------------------------------------------------------------
[[6823,1],2]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: exec5

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
[exec5:24832] 31 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[exec5:24832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[exec10][[6823,1],21][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
[exec2][[6823,1],28][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
[exec1][[6823,1],29][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
[exec8][[6823,1],7][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
[exec4][[6823,1],8][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)[exec4][[6823,1],9][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)

[exec5][[6823,1],0][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] [exec5][[6823,1],1][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.13 failed: Connection refused (111)[exec5][[6823,1],2][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.13 failed: Connection refused (111)connect() to 192.168.0.13 failed: Connection refused (111)


[exec5][[6823,1],3][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.13 failed: Connection refused (111)
[exec2][[6823,1],26][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
[exec2][[6823,1],27][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.0.16 failed: Connection refused (111)
--------------------------------------------------------------------------
A daemon (pid 24835) died unexpectedly with status 137 while attempting
to launch so we are aborting.

There may be more information reported by the environment (see above).

This may be because the daemon was unable to find all the needed shared
libraries on the remote node. You may set your LD_LIBRARY_PATH to have the
location of the shared libraries on the remote nodes and this will
automatically be forwarded to the remote nodes.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun was unable to cleanly terminate the daemons on the nodes shown
below. Additional manual cleanup may be required - please refer to
the "orte-clean" tool for assistance.
--------------------------------------------------------------------------
	exec9.cluster.stats.local
	exec3.cluster.stats.local
	exec6.cluster.stats.local
[exec10:25923] [[6823,0],5] routed:binomial: Connection to lifeline [[6823,0],0] lost

real	360m0.913s
user	987m25.970s
sys	92m32.000s
[exec2:11258] [[6823,0],7] routed:binomial: Connection to lifeline [[6823,0],0] lost
[exec1:13719] [[6823,0],8] routed:binomial: Connection to lifeline [[6823,0],0] lost
[exec8:07856] [[6823,0],1] routed:binomial: Connection to lifeline [[6823,0],0] lost
[exec4:05325] [[6823,0],2] routed:binomial: Connection to lifeline [[6823,0],0] lost
